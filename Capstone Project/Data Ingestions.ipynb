{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1664923e-f118-474f-b346-9b72d502e4df",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Config"
    }
   },
   "outputs": [],
   "source": [
    "#define varibles use in the scripts\n",
    "catalog_name = \"capstone_aimie_dbk\"\n",
    "schema_name = \"medisure\"\n",
    "\n",
    "#storage path\n",
    "input_path = f\"/Volumes/{catalog_name}/{schema_name}/inputs\"\n",
    "schem_path = f\"/Volumes/{catalog_name}/{schema_name}/schem\"\n",
    "bronze_path = f\"/Volumes/{catalog_name}/{schema_name}/schem/bronze\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f65b6a85-3c4f-4486-88ac-da9a6dfcac34",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Schema and Volume"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {schema_name}\")\n",
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS {catalog_name}.{schema_name}.inputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12e22dd7-87f9-4b73-adfe-b75c69c5f335",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Phase: CSVs to Bronze"
    }
   },
   "outputs": [],
   "source": [
    "#creation of Bronze tables \n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType, DateType, ArrayType, BooleanType\n",
    "\n",
    "# Define explicit schemas for enforcement\n",
    "claims_batch_schema = StructType([\n",
    "    StructField(\"ClaimID\", StringType(), True),\n",
    "    StructField(\"MemberID\", StringType(), True),\n",
    "    StructField(\"ProviderID\", StringType(), True),\n",
    "    StructField(\"ClaimDate\", StringType(), True),       \n",
    "    StructField(\"ServiceDate\", StringType(), True),    \n",
    "    StructField(\"Amount\", DoubleType(), True),\n",
    "    StructField(\"Status\", StringType(), True),\n",
    "    StructField(\"ICD10Codes\", StringType(), True),       \n",
    "    StructField(\"CPTCodes\", StringType(), True),        \n",
    "    StructField(\"ClaimType\", StringType(), True),\n",
    "    StructField(\"SubmissionChannel\", StringType(), True),\n",
    "    StructField(\"Notes\", StringType(), True),\n",
    "    StructField(\"IngestTimestamp\", StringType(), True)   \n",
    "])\n",
    "\n",
    "claims_stream_schema = StructType([\n",
    "    StructField(\"ClaimID\", StringType(), True),\n",
    "    StructField(\"MemberID\", StringType(), True),\n",
    "    StructField(\"ProviderID\", StringType(), True),\n",
    "    StructField(\"ClaimDate\", StringType(), True),  \n",
    "    StructField(\"Amount\", DoubleType(), True),\n",
    "    StructField(\"Status\", StringType(), True),\n",
    "    StructField(\"ICD10Codes\", StringType(), True), \n",
    "    StructField(\"CPTCodes\", StringType(), True),   \n",
    "    StructField(\"EventTimestamp\", StringType(), True)\n",
    "])\n",
    "\n",
    "diagnosis_schema = StructType([\n",
    "    StructField(\"Code\", StringType(), True),\n",
    "    StructField(\"Description\", StringType(), True)\n",
    "])\n",
    "\n",
    "members_schema = StructType([\n",
    "    StructField(\"MemberID\", StringType(), True),\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"DOB\", StringType(), True),               \n",
    "    StructField(\"Gender\", StringType(), True),\n",
    "    StructField(\"Region\", StringType(), True),\n",
    "    StructField(\"PlanType\", StringType(), True),\n",
    "    StructField(\"EffectiveDate\", StringType(), True),     \n",
    "    StructField(\"Email\", StringType(), True),\n",
    "    StructField(\"IsActive\", DoubleType(), True),         \n",
    "    StructField(\"LastUpdated\", StringType(), True)      \n",
    "])\n",
    "\n",
    "providers_schema = StructType([\n",
    "    StructField(\"ProviderID\", StringType(), True),\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Specialties\", ArrayType(StringType()), True),\n",
    "    StructField(\"Locations\", ArrayType(StructType([\n",
    "        StructField(\"Address\", StringType(), True),\n",
    "        StructField(\"City\", StringType(), True),\n",
    "        StructField(\"State\", StringType(), True)\n",
    "    ])), True),\n",
    "    StructField(\"IsActive\", BooleanType(), True),\n",
    "    StructField(\"TIN\", StringType(), True),\n",
    "    StructField(\"LastVerified\", DateType(), True)\n",
    "])\n",
    "\n",
    "bronze_claims_batch_df = (spark.read\n",
    "            .format(\"csv\")\n",
    "            .schema(claims_batch_schema)\n",
    "            .option(\"header\", \"true\")\n",
    "            .option(\"timestampFormat\", \"yyyy-MM-dd[ HH:mm:ss]\")\n",
    "            .load(f\"{input_path}/claims_batch.csv\"))\n",
    "\n",
    "bronze_claims_stream_df = (spark.read\n",
    "                    .schema(claims_stream_schema)\n",
    "                    .json(f\"{input_path}/claims_stream.json\"))\n",
    "\n",
    "bronze_diagnosis_df = (spark.read\n",
    "               .format(\"csv\")\n",
    "               .schema(diagnosis_schema)\n",
    "               .option(\"header\", \"true\")\n",
    "               .load(f\"{input_path}/diagnosis_ref.csv\"))\n",
    "\n",
    "bronze_members_df = (spark.read\n",
    "                .format(\"csv\")\n",
    "                .schema(members_schema)\n",
    "                .option(\"header\", \"true\")\n",
    "                .load(f\"{input_path}/members.csv\"))\n",
    "\n",
    "bronze_providers_df = (spark.read\n",
    "                    .schema(providers_schema)\n",
    "                    .json(f\"{input_path}/providers.json\"))\n",
    "\n",
    "(bronze_claims_batch_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"medisure.bronze_claims_batch\"))\n",
    "(bronze_claims_stream_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"medisure.bronze_claims_stream\"))\n",
    "(bronze_diagnosis_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"medisure.bronze_diagnosis_ref\"))\n",
    "(bronze_members_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"medisure.bronze_members\"))\n",
    "(bronze_providers_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"medisure.bronze_providers\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09e86aa8-e7bb-4d84-a44e-b9d8a3c2dc82",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Phase: Bronze to Silver"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, explode, expr\n",
    "\n",
    "silver_claims_batch_df = (\n",
    "    bronze_claims_batch_df\n",
    "    .withColumn(\"ClaimDate\", col(\"ClaimDate\").cast(\"date\"))\n",
    "    .withColumn(\"ServiceDate\", col(\"ServiceDate\").cast(\"date\"))\n",
    "    .withColumn(\"IngestTimestamp\", col(\"IngestTimestamp\").cast(\"timestamp\"))\n",
    "    .withColumn(\"ICD10Code\", explode(expr(\"split(ICD10Codes, ',')\")))\n",
    "    .withColumn(\"CPTCode\", explode(expr(\"split(CPTCodes, ',')\")))\n",
    "    .select(\n",
    "        col(\"ClaimID\"),\n",
    "        col(\"MemberID\"),\n",
    "        col(\"ProviderID\"),\n",
    "        col(\"ClaimDate\"),\n",
    "        col(\"Amount\"),\n",
    "        col(\"Status\"),\n",
    "        col(\"ICD10Code\").alias(\"ICD10Codes\"),\n",
    "        col(\"CPTCode\").alias(\"CPTCodes\"),\n",
    "        col(\"IngestTimestamp\")\n",
    "    )\n",
    ").dropna(subset=[\"ClaimID\", \"MemberID\", \"ProviderID\"]).dropDuplicates([\"ClaimID\", \"MemberID\", \"ProviderID\"]).filter((col(\"ClaimID\").isNotNull()) & (col(\"MemberID\").isNotNull()) & (col(\"ProviderID\").isNotNull()))\n",
    "\n",
    "silver_claims_stream_df = (\n",
    "    bronze_claims_stream_df\n",
    "    .withColumn(\"ClaimDate\", col(\"ClaimDate\").cast(\"date\"))\n",
    "    .withColumn(\"IngestTimestamp\", col(\"EventTimestamp\").cast(\"timestamp\"))\n",
    "    .select(\n",
    "        col(\"ClaimID\"),\n",
    "        col(\"MemberID\"),\n",
    "        col(\"ProviderID\"),\n",
    "        col(\"ClaimDate\"),\n",
    "        col(\"Amount\"),\n",
    "        col(\"Status\"),\n",
    "        col(\"ICD10Codes\"),\n",
    "        col(\"CPTCodes\"),\n",
    "        col(\"IngestTimestamp\")\n",
    "    )\n",
    ").dropna(subset=[\"ClaimID\", \"MemberID\", \"ProviderID\"]).dropDuplicates([\"ClaimID\", \"MemberID\", \"ProviderID\"]).filter((col(\"ClaimID\").isNotNull()) & (col(\"MemberID\").isNotNull()) & (col(\"ProviderID\").isNotNull()))\n",
    "\n",
    "silver_diagnosis_df = (\n",
    "    bronze_diagnosis_df\n",
    ").dropna(subset=[\"Code\"]).dropDuplicates([\"Code\"]).filter((col(\"Code\").isNotNull()) & (col(\"Description\").isNotNull()))\n",
    "\n",
    "silver_members_df = (\n",
    "    bronze_members_df\n",
    "    .withColumn(\"DOB\", col(\"DOB\").cast(\"date\"))\n",
    "    .withColumn(\"EffectiveDate\", col(\"EffectiveDate\").cast(\"date\"))\n",
    "    .withColumn(\"LastUpdated\", col(\"LastUpdated\").cast(\"date\"))\n",
    ").dropna(subset=[\"MemberID\"]).dropDuplicates([\"MemberID\"]).filter((col(\"MemberID\").isNotNull()))\n",
    "\n",
    "silver_providers_df = (\n",
    "    bronze_providers_df\n",
    "    .withColumn(\"LastVerified\", col(\"LastVerified\").cast(\"date\"))\n",
    "    .withColumn(\"Specialties\", explode(col(\"Specialties\")))\n",
    "    .withColumn(\"Location\", explode(col(\"Locations\")))\n",
    "    .select(\n",
    "        col(\"ProviderID\"),\n",
    "        col(\"Name\").alias(\"ProviderName\"),\n",
    "        col(\"Specialties\"),\n",
    "        col(\"Location.Address\").alias(\"Address\"),\n",
    "        col(\"Location.City\").alias(\"City\"),\n",
    "        col(\"Location.State\").alias(\"State\"),\n",
    "        col(\"IsActive\").alias(\"IsActiveFlag\"),\n",
    "        col(\"TIN\"),\n",
    "        col(\"LastVerified\")\n",
    "    )\n",
    ").dropna(subset=[\"ProviderID\"]).dropDuplicates([\"ProviderID\"]).filter((col(\"ProviderID\").isNotNull()))\n",
    "\n",
    "(silver_claims_batch_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"medisure.silver_claims_batch\"))\n",
    "(silver_claims_stream_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"medisure.silver_claims_stream\"))\n",
    "(silver_diagnosis_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"medisure.silver_diagnosis_ref\"))\n",
    "(silver_members_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"medisure.silver_members\"))\n",
    "(silver_providers_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"medisure.silver_providers\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f028894-86b0-420e-9b5c-32f1d96a543a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Phase: Silver Transform"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, udf, lit\n",
    "\n",
    "# Add source flag to each DataFrame\n",
    "claims_batch_with_source = silver_claims_batch_df.withColumn(\"Source\", lit(\"Batch\"))\n",
    "claims_stream_with_source = silver_claims_stream_df.withColumn(\"Source\", lit(\"Stream\"))\n",
    "\n",
    "# Union the DataFrames\n",
    "combined_claims_df = claims_batch_with_source.unionByName(claims_stream_with_source)\n",
    "\n",
    "(combined_claims_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"medisure.silver_claims_transform\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62423831-36d8-4e0d-824a-201e0f728a14",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Phase: Gold layer"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, udf, lit\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# Define a UDF for fraud scoring\n",
    "@udf(returnType=IntegerType())\n",
    "def fraud_score(amount, status):\n",
    "    if amount is None or status is None:\n",
    "        return 0\n",
    "    if amount > 10000 and status == \"Pending\":\n",
    "        return 1\n",
    "    elif amount > 5000 and status == \"Approved\":\n",
    "        return 0.5\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Join datasets\n",
    "gold_enriched_claims_df = (\n",
    "    combined_claims_df\n",
    "    .join(silver_members_df, \"MemberID\", \"inner\")\n",
    "    .join(silver_providers_df, \"ProviderID\", \"inner\")\n",
    "    .join(silver_diagnosis_df, col(\"ICD10Codes\") == col(\"Code\"), \"left\")\n",
    "    .withColumn(\"FraudScore\", fraud_score(col(\"Amount\"), col(\"Status\")))\n",
    "    .withColumn(\"IsValid\", when((col(\"FraudScore\") > 0) & (col(\"Status\") == \"Pending\"), \"No\")\n",
    "                          .when((col(\"FraudScore\") == 0) & (col(\"Status\") == \"Approved\"), \"Yes\")\n",
    "                          .otherwise(\"Unknown\"))\n",
    ")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS medisure.gold_enriched_claims (\n",
    "    ClaimID STRING,\n",
    "    MemberID STRING,\n",
    "    ProviderID STRING,\n",
    "    ClaimDate DATE,\n",
    "    Amount DOUBLE,\n",
    "    Status STRING,\n",
    "    ICD10Codes STRING,\n",
    "    CPTCodes STRING,\n",
    "    IngestTimestamp TIMESTAMP,\n",
    "    Source STRING,\n",
    "    Name STRING,\n",
    "    DOB DATE,\n",
    "    Gender STRING,\n",
    "    Region STRING,\n",
    "    PlanType STRING,\n",
    "    EffectiveDate DATE,\n",
    "    Email STRING,\n",
    "    IsActive DOUBLE,\n",
    "    LastUpdated DATE,\n",
    "    ProviderName STRING,\n",
    "    Specialties STRING,\n",
    "    Address STRING,\n",
    "    City STRING,\n",
    "    State STRING,\n",
    "    IsActiveFlag BOOLEAN,\n",
    "    TIN STRING,\n",
    "    LastVerified DATE,\n",
    "    Description STRING,\n",
    "    FraudScore INT,\n",
    "    IsValid STRING\n",
    ")\n",
    "USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Merge into the gold_enriched_claims table\n",
    "gold_enriched_claims_df.createOrReplaceTempView(\"gold_enriched_claims_df\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "MERGE INTO medisure.gold_enriched_claims AS target\n",
    "USING gold_enriched_claims_df AS source\n",
    "ON target.ClaimID = source.ClaimID\n",
    "WHEN MATCHED THEN\n",
    "  UPDATE SET *\n",
    "WHEN NOT MATCHED THEN\n",
    "  INSERT *\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6463515690840685,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Data Ingestions",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
