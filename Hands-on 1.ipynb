{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f438957-0653-4cca-87ea-9b164f7d37d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    " \n",
    "# Read CSV with multiline and escape options to handle JSON in ProductMetadata\n",
    "sales_dfv1 = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"multiLine\", \"true\")\n",
    "    .option(\"escape\", \"\\\"\")\n",
    "    .option(\"quote\", \"\\\"\")\n",
    "    .load(\"/Volumes/workspace/default/data_volume/new_sales.csv\")\n",
    ")\n",
    " \n",
    "display(sales_dfv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abfc02ee-9d57-4085-8022-06b2e67e9034",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load CSV with inferSchema\n",
    "sales_df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"inferSchema\", True) \\\n",
    "    .load(\"dbfs:/FileStore/data/new_sales.csv\")\n",
    "\n",
    "print(\"Initial data sample:\")\n",
    "sales_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edf6a14d-5996-41a0-ae4b-860bed503d8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# Remove duplicates\n",
    "sales_df_clean = sales_df.dropDuplicates()\n",
    "\n",
    "# Drop rows with missing Quantity or CustomerName (critical fields)\n",
    "sales_df_clean = sales_df_clean.dropna(subset=[\"Quantity\", \"CustomerName\"])\n",
    "\n",
    "# Fix data types - convert Quantity to integer safely\n",
    "sales_df_clean = sales_df_clean.withColumn(\n",
    "    \"Quantity\",\n",
    "    when(col(\"Quantity\").cast(\"int\").isNotNull(), col(\"Quantity\").cast(\"int\")).otherwise(None)\n",
    ")\n",
    "\n",
    "# Convert UnitPrice to float, set invalid to null\n",
    "sales_df_clean = sales_df_clean.withColumn(\n",
    "    \"UnitPrice\",\n",
    "    when(col(\"UnitPrice\").cast(\"float\").isNotNull(), col(\"UnitPrice\").cast(\"float\")).otherwise(None)\n",
    ")\n",
    "\n",
    "# Convert TaxAmount to float\n",
    "sales_df_clean = sales_df_clean.withColumn(\n",
    "    \"TaxAmount\",\n",
    "    when(col(\"TaxAmount\").cast(\"float\").isNotNull(), col(\"TaxAmount\").cast(\"float\")).otherwise(None)\n",
    ")\n",
    "\n",
    "# Drop rows where conversions failed (null in Quantity or UnitPrice)\n",
    "sales_df_clean = sales_df_clean.dropna(subset=[\"Quantity\", \"UnitPrice\"])\n",
    "\n",
    "print(\"Cleaned data sample:\")\n",
    "sales_df_clean.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7cfa5c6-9ade-4a83-8e4b-dbb8c89d5493",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write cleaned data as Delta table\n",
    "sales_df_clean.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"sales_delta_cleaned\")\n",
    "\n",
    "print(\"Cleaned Delta table created: sales_delta_cleaned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a037aace-3581-460c-8b32-4021150249ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Load cleaned Delta table\n",
    "df = spark.table(\"sales_delta_cleaned\")\n",
    "\n",
    "# Add TotalPrice column\n",
    "df = df.withColumn(\"TotalPrice\", col(\"Quantity\") * col(\"UnitPrice\"))\n",
    "\n",
    "# Filter rows where Quantity > 5\n",
    "filtered_df = df.filter(col(\"Quantity\") > 5)\n",
    "\n",
    "filtered_df.select(\n",
    "    \"SalesOrderNumber\", \"CustomerName\", \"Item\", \"Quantity\", \"UnitPrice\", \"TotalPrice\"\n",
    ").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c090dc70-d1f1-4f57-869f-5291601b68f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Total sales per customer\n",
    "SELECT CustomerName, SUM(Quantity) AS TotalQuantity, SUM(Quantity * UnitPrice) AS TotalSales\n",
    "FROM sales_delta_cleaned\n",
    "GROUP BY CustomerName\n",
    "ORDER BY TotalSales DESC\n",
    "LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75b16207-cee9-4a45-a276-6c1fdc4737f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"color\", StringType(), True),\n",
    "    StructField(\"warranty\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_with_json = df.withColumn(\"ProductDetails\", from_json(col(\"ProductMetadata\"), schema))\n",
    "\n",
    "df_with_json.select(\n",
    "    \"SalesOrderNumber\", \"ProductMetadata\", \"ProductDetails.color\", \"ProductDetails.warranty\"\n",
    ").show(5)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5518772679146698,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Hands-on 1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
