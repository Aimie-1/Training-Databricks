{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7333a03b-77f5-4e2e-a9b0-d9727b89d084",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_path = \"dbfs:/FileStore/data/new_sales.csv\"\n",
    "\n",
    "sales_df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(csv_path)\n",
    "\n",
    "print(\"Raw data sample:\")\n",
    "sales_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7d2caf6-d62e-41ae-869c-97c328772dc2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "clean_df = sales_df.dropDuplicates() \\\n",
    "    .dropna(subset=[\"Quantity\", \"CustomerName\"]) \\\n",
    "    .withColumn(\"Quantity\", col(\"Quantity\").cast(\"int\")) \\\n",
    "    .withColumn(\"UnitPrice\", col(\"UnitPrice\").cast(\"float\"))\n",
    "\n",
    "print(\"Cleaned data preview:\")\n",
    "clean_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8269960-5cc7-4f78-b782-af775d184a71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json, regexp_replace\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "# Clean JSON string in ProductMetadata\n",
    "clean_df = clean_df.withColumn(\n",
    "    \"ProductMetadata_clean\",\n",
    "    regexp_replace(col(\"ProductMetadata\"), '^\"+|\"+$', '')\n",
    ").withColumn(\n",
    "    \"ProductMetadata_clean\",\n",
    "    regexp_replace(col(\"ProductMetadata_clean\"), '\"\"', '\"')\n",
    ")\n",
    "\n",
    "# Define schema for JSON parsing\n",
    "json_schema = StructType([\n",
    "    StructField(\"color\", StringType(), True),\n",
    "    StructField(\"warranty\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Parse JSON column\n",
    "clean_df = clean_df.withColumn(\"ProductDetails\", from_json(col(\"ProductMetadata_clean\"), json_schema))\n",
    "\n",
    "# Extract fields\n",
    "clean_df = clean_df.withColumn(\"color\", col(\"ProductDetails.color\")) \\\n",
    "    .withColumn(\"warranty\", col(\"ProductDetails.warranty\"))\n",
    "\n",
    "print(\"Data with parsed JSON fields:\")\n",
    "clean_df.select(\"SalesOrderNumber\", \"color\", \"warranty\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "827d5d6e-d0d3-474a-9434-bb68947cc18c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "clean_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"sales_cleaned\")\n",
    "print(\"Delta table 'sales_cleaned' created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c25ca630-0168-4ee0-8db1-086e2d8624ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum as _sum\n",
    "\n",
    "agg_df = clean_df.groupBy(\"CustomerName\").agg(\n",
    "    _sum(col(\"Quantity\") * col(\"UnitPrice\")).alias(\"TotalSales\")\n",
    ")\n",
    "\n",
    "print(\"Aggregated sales per customer:\")\n",
    "agg_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b9b782f-3d73-4991-b007-228cb6d69e64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, DateType, IntegerType, DoubleType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"SalesOrderNumber\", StringType(), True),\n",
    "    StructField(\"OrderDate\", DateType(), True),\n",
    "    StructField(\"CustomerName\", StringType(), True),\n",
    "    StructField(\"EmailAddress\", StringType(), True),\n",
    "    StructField(\"Item\", StringType(), True),\n",
    "    StructField(\"Quantity\", IntegerType(), True),\n",
    "    StructField(\"UnitPrice\", DoubleType(), True),\n",
    "    StructField(\"TaxAmount\", DoubleType(), True),\n",
    "    StructField(\"ProductMetadata\", StringType(), True)\n",
    "])\n",
    "\n",
    "sales_df = (\n",
    "    spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"csv\") \n",
    "    .option(\"cloudFiles.schemaLocation\", \"/FileStore/data/sales/\")\n",
    "    .schema(schema)\n",
    "    .load(\"/FileStore/data/sales/\")\n",
    ")\n",
    "\n",
    "deduped_df = sales_df.dropDuplicates([\"SalesOrderNumber\"])\n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "(\n",
    "    deduped_df.writeStream\n",
    "    .foreachBatch(\n",
    "        lambda batch_df, epoch_id: (\n",
    "            DeltaTable.forName(spark, \"sales_raw\")\n",
    "            .alias(\"target\")\n",
    "            .merge(\n",
    "                batch_df.alias(\"source\"),\n",
    "                \"target.SalesOrderNumber = source.SalesOrderNumber\"\n",
    "            )\n",
    "            .whenNotMatchedInsertAll()\n",
    "            .execute()\n",
    "        )\n",
    "    )\n",
    "    .outputMode(\"update\")\n",
    "    .option(\"checkpointLocation\", \"/FileStore/data/sales/\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "\n",
    "display(sales_df.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ec239a9-9273-48eb-bf2e-0b7208759496",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_path = \"dbfs:/FileStore/data/new_sales*.csv\"\n",
    "\n",
    "sales_df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(csv_path)\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "clean_df = sales_df.dropDuplicates()\n",
    "\n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "\n",
    "(\n",
    "    deduped_df.writeStream\n",
    "    .foreachBatch(\n",
    "        lambda batch_df, epoch_id: (\n",
    "            DeltaTable.forName(spark, \"sales_raw\")\n",
    "            .alias(\"target\")\n",
    "            .merge(\n",
    "                batch_df.alias(\"source\"),\n",
    "                \"target.SalesOrderNumber = source.SalesOrderNumber\"\n",
    "            )\n",
    "            .whenNotMatchedInsertAll()\n",
    "            .execute()\n",
    "        )\n",
    "    )\n",
    "    .outputMode(\"update\")\n",
    "    .option(\"checkpointLocation\", \"/mnt/checkpoints/sales_raw/\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8793b09-d5fe-4a97-8b7c-46e37c25b644",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import input_file_name\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType\n",
    "\n",
    "# Define schema for sales data\n",
    "schema = StructType([\n",
    "    StructField(\"SalesOrderNumber\", StringType(), True),\n",
    "    StructField(\"OrderDate\", DateType(), True),\n",
    "    StructField(\"CustomerName\", StringType(), True),\n",
    "    StructField(\"EmailAddress\", StringType(), True),\n",
    "    StructField(\"Item\", StringType(), True),\n",
    "    StructField(\"Quantity\", IntegerType(), True),\n",
    "    StructField(\"UnitPrice\", DoubleType(), True),\n",
    "    StructField(\"TaxAmount\", DoubleType(), True),\n",
    "    StructField(\"ProductMetadata\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Simulate daily CSV files landing in /mnt/sales_landing/\n",
    "# (In practice, files would be uploaded externally or via dbutils.fs.cp)\n",
    "\n",
    "# List all CSV files in the landing folder\n",
    "landing_path = \"/FileStore/data/sales/\"\n",
    "all_files = [f.path for f in dbutils.fs.ls(landing_path) if f.path.endswith(\".csv\")]\n",
    "\n",
    "# Read already ingested file list from Delta table metadata\n",
    "if spark._jsparkSession.catalog().tableExists(\"sales_raw\"):\n",
    "    ingested_files = spark.read.format(\"delta\").table(\"sales_raw\").select(\"source_file\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "else:\n",
    "    ingested_files = []\n",
    "\n",
    "# Detect new files\n",
    "new_files = [f for f in all_files if f not in ingested_files]\n",
    "\n",
    "if new_files:\n",
    "    # Read new files with schema validation\n",
    "    new_df = (\n",
    "        spark.read\n",
    "        .option(\"header\", True)\n",
    "        .schema(sales_schema)\n",
    "        .csv(new_files)\n",
    "        .withColumn(\"source_file\", input_file_name())\n",
    "    )\n",
    "\n",
    "    # Append new data to sales_raw Delta table, avoiding duplicates by OrderID and source_file\n",
    "    if spark._jsparkSession.catalog().tableExists(\"sales_raw\"):\n",
    "        sales_raw_df = spark.read.format(\"delta\").table(\"sales_raw\")\n",
    "        new_df = new_df.join(\n",
    "            sales_raw_df.select(\"OrderID\", \"source_file\"),\n",
    "            on=[\"OrderID\", \"source_file\"],\n",
    "            how=\"left_anti\"\n",
    "        )\n",
    "        new_df.write.format(\"delta\").mode(\"append\").saveAsTable(\"sales_raw\")\n",
    "    else:\n",
    "        new_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"sales_raw\")\n",
    "\n",
    "display(spark.read.table(\"sales_raw\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d536ad23-b254-4978-beaa-8182d7b56465",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, avg, when, to_date, regexp_replace\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Read raw sales data\n",
    "raw_df = spark.read.table(\"sales_raw\")\n",
    "\n",
    "# 1. Correct inconsistent date formats in OrderDate\n",
    "# Try parsing as yyyy-MM-dd, then as MM/dd/yyyy if null\n",
    "cleaned_dates_df = raw_df.withColumn(\n",
    "    \"OrderDate\",\n",
    "    when(\n",
    "        to_date(col(\"OrderDate\"), \"yyyy-MM-dd\").isNotNull(),\n",
    "        to_date(col(\"OrderDate\"), \"yyyy-MM-dd\")\n",
    "    ).otherwise(\n",
    "        to_date(col(\"OrderDate\"), \"MM/dd/yyyy\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# 2. Impute missing UnitPrice with average per Product (Item)\n",
    "window_spec = Window.partitionBy(\"Product\")\n",
    "imputed_df = cleaned_dates_df.withColumn(\n",
    "    \"UnitPrice\",\n",
    "    when(\n",
    "        col(\"UnitPrice\").isNull(),\n",
    "        avg(col(\"UnitPrice\")).over(window_spec)\n",
    "    ).otherwise(col(\"UnitPrice\"))\n",
    ")\n",
    "\n",
    "# 3. Remove or flag suspicious records (Quantity â‰¤ 0)\n",
    "final_clean_df = imputed_df.withColumn(\n",
    "    \"is_suspicious\",\n",
    "    when(col(\"Quantity\") <= 0, True).otherwise(False)\n",
    ").filter(col(\"Quantity\") > 0)\n",
    "\n",
    "display(final_clean_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd092745-cdf6-4fe0-99dd-72b598a67063",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum as _sum, rank, avg as _avg, col, lag\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Assume 'final_clean_df' is available from previous cell\n",
    "\n",
    "# 1. Running total of sales per customer sorted by OrderDate\n",
    "running_total_window = Window.partitionBy(\"CustomerName\").orderBy(\"OrderDate\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "df_running_total = final_clean_df.withColumn(\n",
    "    \"RunningTotalSales\",\n",
    "    _sum(col(\"Quantity\") * col(\"UnitPrice\")).over(running_total_window)\n",
    ")\n",
    "\n",
    "# 2. Rank of customers by total sales per region\n",
    "# Assume 'Region' column exists; if not, replace 'Region' with appropriate column or remove partitioning by region\n",
    "total_sales_df = final_clean_df.groupBy(\"Region\", \"CustomerName\").agg(\n",
    "    _sum(col(\"Quantity\") * col(\"UnitPrice\")).alias(\"TotalSales\")\n",
    ")\n",
    "rank_window = Window.partitionBy(\"Region\").orderBy(col(\"TotalSales\").desc())\n",
    "df_ranked = total_sales_df.withColumn(\n",
    "    \"CustomerRank\",\n",
    "    rank().over(rank_window)\n",
    ")\n",
    "\n",
    "# 3. Identify customers with a sudden spike in purchases compared to their average\n",
    "# Calculate average sales per customer\n",
    "avg_sales_window = Window.partitionBy(\"CustomerName\")\n",
    "# Calculate previous day's sales per customer\n",
    "order_window = Window.partitionBy(\"CustomerName\").orderBy(\"OrderDate\")\n",
    "df_spike = final_clean_df.withColumn(\n",
    "    \"DailySales\", col(\"Quantity\") * col(\"UnitPrice\")\n",
    ").withColumn(\n",
    "    \"AvgSales\", _avg(\"DailySales\").over(avg_sales_window)\n",
    ").withColumn(\n",
    "    \"PrevSales\", lag(\"DailySales\").over(order_window)\n",
    ").withColumn(\n",
    "    \"Spike\",\n",
    "    (col(\"DailySales\") > 2 * col(\"AvgSales\")) & (col(\"PrevSales\").isNotNull())\n",
    ")\n",
    "\n",
    "display(df_running_total)\n",
    "display(df_ranked)\n",
    "display(df_spike.filter(col(\"Spike\")))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Lab 2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
